{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3441630,"sourceType":"datasetVersion","datasetId":2073174},{"sourceId":289957,"sourceType":"modelInstanceVersion","modelInstanceId":246484,"modelId":268077}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt\n!sudo apt-get install -y tensorrt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorrt\nprint(tensorrt.__version__)\nassert tensorrt.Builder(tensorrt.Logger())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get install python3-libnvinfer-dev -y\n!python3 -m pip install numpy onnx\n!sudo apt-get install onnx-graphsurgeon -y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!dpkg-query -W tensorrt\n!dpkg -l | grep nvinfer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade sphinx-glpi-theme\n!pip install --no-cache-dir --index-url https://pypi.nvidia.com pytorch-quantization\n!pip install wget\n!pip3 install -U pip && pip3 install onnxsim\n!pip install onnxruntime","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:29:50.160011Z","iopub.execute_input":"2025-03-17T19:29:50.160443Z","iopub.status.idle":"2025-03-17T19:29:50.170308Z","shell.execute_reply.started":"2025-03-17T19:29:50.160356Z","shell.execute_reply":"2025-03-17T19:29:50.169393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models, datasets\nfrom torch.utils.tensorboard import SummaryWriter\nimport onnxruntime\nimport onnxsim\n\n\nimport pytorch_quantization\nfrom pytorch_quantization import nn as quant_nn\nfrom pytorch_quantization import quant_modules\nfrom pytorch_quantization import calib\nfrom pytorch_quantization.tensor_quant import QuantDescriptor\nfrom pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer\nfrom pytorch_quantization.nn.modules import quant_pooling\nfrom tqdm import tqdm\n\nprint(pytorch_quantization.__version__)\n# print(torch_tensorrt.__version__)\nprint(onnxruntime.__version__)\nprint(onnxsim.__version__)\n#print(torchaudio.__version__)\n\nimport os\nimport sys\nimport warnings\nimport time\nimport numpy as np\nimport wget\nimport tarfile\nimport shutil\nwarnings.simplefilter('ignore')\nimport collections\n\n# Accelerate parts\nfrom accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\nfrom accelerate.utils import set_seed # reproducability across devices\ntorch.backends.cudnn.benchmark = True  # Enables auto-tuning for speed optimization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:29:53.096384Z","iopub.execute_input":"2025-03-17T19:29:53.097556Z","iopub.status.idle":"2025-03-17T19:29:54.597247Z","shell.execute_reply.started":"2025-03-17T19:29:53.097502Z","shell.execute_reply":"2025-03-17T19:29:54.596078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the Accelerator\naccelerator = Accelerator()\n\n# Set seed for reproducibility\nset_seed(42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport os\nimport random\n\nclass CityscapesDataset(Dataset):\n    def __init__(self, root_dir, split='train', resize_shape=(512, 1024), is_train = True):\n        super().__init__()\n        self.inputs, self.targets = [], []\n        self.resize_shape = resize_shape\n        self.ignore_index = 255\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_map = {k: v for v, k in enumerate(self.valid_classes)}\n        self.is_train = is_train\n\n        img_dir = os.path.join(root_dir, 'leftImg8bit', split)\n        mask_dir = os.path.join(root_dir, 'gtFine', split)\n\n        for root, _, filenames in os.walk(img_dir):\n            for filename in filenames:\n                if filename.endswith('.png'):\n                    filename_base = '_'.join(filename.split('_')[:-1])\n                    input_path = os.path.join(root, filename_base + '_leftImg8bit.png')\n                    target_path = os.path.join(mask_dir, os.path.basename(root), filename_base + '_gtFine_labelIds.png')\n\n                    if os.path.exists(input_path) and os.path.exists(target_path):\n                        self.inputs.append(input_path)\n                        self.targets.append(target_path)\n\n        print(f\"Found {len(self.inputs)} images and {len(self.targets)} targets for split '{split}'\")\n\n        # Define Albumentations transformation pipeline\n        if self.is_train:\n            self.transform = A.Compose([\n                A.RandomResizedCrop(self.resize_shape[0], self.resize_shape[1], scale=(0.7, 1.0), ratio=(1.0, 1.0), p=0.5),\n                A.RandomScale(scale_limit=0.3, p=0.5),  # Scale images to enhance small objects\n                A.HorizontalFlip(p=0.5),  # Useful for symmetrical structures\n                A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, p=0.5),  # More aggressive color variation\n                A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n                A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),  # Helps with perspective issues\n                A.CoarseDropout(max_holes=12, max_height=75, max_width=75, p=0.6),  # Simulate occlusions\n                A.Resize(self.resize_shape[0], self.resize_shape[1], p=1.0),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = A.Compose([\n                A.Resize(self.resize_shape[0], self.resize_shape[1]),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2()\n            ])\n\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, i):\n        input_img = np.array(Image.open(self.inputs[i]).convert('RGB'))\n        target_img = np.array(Image.open(self.targets[i]))\n\n        # Apply transformations to both image and mask\n        transformed = self.transform(image=input_img, mask=target_img)\n        input_img = transformed['image']\n        target_tensor = transformed['mask']\n\n        # Remap target labels\n        remapped_target = torch.full_like(target_tensor, self.ignore_index)\n        for cls, mapped in self.class_map.items():\n            remapped_target[target_tensor == cls] = mapped\n\n        return input_img, remapped_target\n\n# DataLoader setup\nroot_dir = '/kaggle/input/cityscapes/Cityscape'\nresize_shape = (512, 1024)\n\ntrain_dataset = CityscapesDataset(root_dir=root_dir, split='train', resize_shape=resize_shape, is_train=True)\nval_dataset = CityscapesDataset(root_dir=root_dir, split='val', resize_shape=resize_shape,  is_train=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\ninference_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n\n# Load a small subset for calibration\n# Select a subset of 100 images for calibration\nCALIBRATION_SIZE = 500\nrandom.seed(42)\ncalib_indices = random.sample(range(len(val_dataset)), CALIBRATION_SIZE)\ncalib_subset = torch.utils.data.Subset(val_dataset, calib_indices)\n# calib_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'val'), transform=data_transforms['val'])\ncalib_dataloader = torch.utils.data.DataLoader(\n    calib_subset, batch_size=8, shuffle=False, drop_last=True, pin_memory=True, num_workers=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:01.796427Z","iopub.execute_input":"2025-03-17T19:30:01.797316Z","iopub.status.idle":"2025-03-17T19:30:07.207148Z","shell.execute_reply.started":"2025-03-17T19:30:01.797282Z","shell.execute_reply":"2025-03-17T19:30:07.206134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import resnet18\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom collections import OrderedDict\n\nnum_classes = 19  # Cityscapes has 19 classes\n\n# Define FCNHead manually (since torchvision does not expose it)\nclass FCNHead(nn.Sequential):\n    def __init__(self, in_channels, num_classes):\n        super().__init__(\n            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3),  # Prevents overfitting\n            nn.Conv2d(256, num_classes, kernel_size=1)\n        )\n\n# Load ResNet-18 backbone\ntry:\n    backbone = resnet18(weights=\"IMAGENET1K_V1\")  # Torchvision >= 0.13\nexcept TypeError:\n    backbone = resnet18(pretrained=True)  # Older torchvision versions\n\n# Extract only the feature layers (excluding final FC layer)\nreturn_layers = {\"layer4\": \"out\"}  # Extract feature maps from layer4\nbackbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n\n# Load pre-trained FCN-ResNet50 to initialize classifier weights\nfcn_resnet50 = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n\n# Create FCN model with ResNet-18 backbone\nmodel = torchvision.models.segmentation.fcn_resnet50(pretrained=False, num_classes=num_classes)\n\n# Replace the backbone with ResNet-18\nmodel.backbone = backbone\n\n# **Fix: Initialize a new FCNHead with correct input channels (512)**\nmodel.classifier = FCNHead(512, num_classes)  # ResNet-18 outputs 512 channels\n\n# **Fix: Do not transfer classifier weights from ResNet-50 (1024 â†’ 512 mismatch)**\n\n# **Optional:** Remove auxiliary classifier if it causes issues\nmodel.aux_classifier = None\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(\"âœ… FCN-ResNet18 Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:09.133898Z","iopub.execute_input":"2025-03-17T19:30:09.134252Z","iopub.status.idle":"2025-03-17T19:30:12.832368Z","shell.execute_reply.started":"2025-03-17T19:30:09.134222Z","shell.execute_reply":"2025-03-17T19:30:12.831336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Decode function to convert mask to RGB (Cityscapes)\ndef decode_segmap(mask, n_classes=19, label_colours=None):\n    if label_colours is None:\n        label_colours = [\n            (128, 64, 128), (244, 35, 232), (70, 70, 70), (102, 102, 156),\n            (190, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n            (107, 142, 35), (152, 251, 152), (0, 130, 180), (220, 20, 60),\n            (255, 0, 0), (0, 0, 142), (0, 0, 70), (0, 60, 100),\n            (0, 80, 100), (0, 0, 230), (119, 11, 32)\n        ]  # 19 valid classes\n\n    if isinstance(mask, torch.Tensor):\n        mask = mask.cpu().numpy()\n\n    # Ensure ignored pixels (255) are set to zero\n    mask[mask == 255] = 0\n\n    r, g, b = np.zeros_like(mask, dtype=np.uint8), np.zeros_like(mask, dtype=np.uint8), np.zeros_like(mask, dtype=np.uint8)\n\n    for l in range(n_classes):\n        r[mask == l] = label_colours[l][0]\n        g[mask == l] = label_colours[l][1]\n        b[mask == l] = label_colours[l][2]\n\n    return np.stack([r, g, b], axis=2)  # Convert to RGB image\n\n# Function to visualize input, ground truth, and predicted masks\ndef visualize_segmentation(input_image, decoded_target=None, decoded_prediction=None):\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    # Convert input tensor to numpy for display\n    img = input_image.cpu().numpy().transpose(1, 2, 0)  # Convert [C, H, W] to [H, W, C]\n    img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n\n    axs[0].imshow(img)\n    axs[0].set_title(\"Input Image\")\n    axs[0].axis('off')\n\n    if decoded_target is not None:\n        axs[1].imshow(decoded_target)\n        axs[1].set_title(\"Ground Truth Mask\")\n        axs[1].axis('off')\n    else:\n        axs[1].imshow(np.zeros_like(img))\n        axs[1].set_title(\"Ground Truth Unavailable\")\n        axs[1].axis('off')\n\n    if decoded_prediction is not None:\n        axs[2].imshow(decoded_prediction)\n        axs[2].set_title(\"Predicted Mask\")\n        axs[2].axis('off')\n    else:\n        axs[2].imshow(np.zeros_like(img))\n        axs[2].set_title(\"Prediction Unavailable\")\n        axs[2].axis('off')\n\n    plt.show()\n\n# Function to check if the model is trained\ndef is_model_trained(model, threshold=0.001):\n    \"\"\"Check if classifier weights are different from initialization.\"\"\"\n    with torch.no_grad():\n        weights = model.classifier[-1].weight\n        return torch.var(weights).item() > threshold  # Check variance instead of mean\n\n\n# Function to perform inference and visualize predictions\ndef visualize_predictions(model, data_loader, device, n_classes=19):\n    model.eval()\n    \n    trained = is_model_trained(model)\n    print(f\"Model trained: {trained}\")\n\n    with torch.no_grad():\n        for input_img, target_mask in data_loader:\n            input_img, target_mask = input_img.to(device), target_mask.to(device)\n\n            if trained:\n                # Perform inference\n                output = model(input_img)\n                if isinstance(output, dict):  # FCN models return a dict with 'out' key\n                    output = output[\"out\"]\n                _, predicted_mask = torch.max(output, 1)  # Get predicted class indices\n            else:\n                predicted_mask = None  # No predictions, show random samples\n\n            # Select single image from batch\n            input_image = input_img[0]\n            target_mask = target_mask[0]\n            predicted_mask = predicted_mask[0] if predicted_mask is not None else None\n\n            # Decode segmentation masks\n            decoded_target = decode_segmap(target_mask, n_classes=n_classes) if target_mask is not None else None\n            decoded_prediction = decode_segmap(predicted_mask, n_classes=n_classes) if predicted_mask is not None else None\n\n            # Visualize the segmentation results\n            visualize_segmentation(input_image, decoded_target, decoded_prediction)\n            break  # Show only one batch\n\n# Ensure model is on GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrained_model = model.to(device)  # Ensure model is on correct device\n\n# Call the visualization function with the model and data loader\nvisualize_predictions(trained_model, inference_loader, device, n_classes=19)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Loss function with ignore_index for unlabeled pixels\ncriterion = nn.CrossEntropyLoss(ignore_index=255)\n\noptimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4) #2e-5\n# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n# scheduler = CosineAnnealingLR(optimizer, T_max=10)  # T_max is the number of epochs before resetting\n\n\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=1, verbose=True)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n\n# Class weighting to handle class imbalance\nclass_weights = torch.tensor([\n    1.0, 1.5, 1.0, 7.0, 4.0, 6.0, 16.0, 5.0, 1.0, 4.5, 1.0, \n    22.0, 22.0, 1.0, 20.0, 22.0, 24.0, 24.0, 24.0  # Increased 11-18 weights\n], device=device).to(device)\n\n\n# Define loss function\n# criterion = FocalLoss(weight=class_weights, gamma=1.5, reduction=\"mean\", ignore_index=255)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=255)  # Ignore unlabeled pixels\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"max\", factor=0.3, patience=2, verbose=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:15.156908Z","iopub.execute_input":"2025-03-17T19:30:15.157846Z","iopub.status.idle":"2025-03-17T19:30:15.165252Z","shell.execute_reply.started":"2025-03-17T19:30:15.157806Z","shell.execute_reply":"2025-03-17T19:30:15.164272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Best Model Checkpoint\ncheckpoint = torch.load(\"/kaggle/input/fcnresnet18/pytorch/default/2/best_fcn_resnet18_ckptv4.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['opt_state_dict'])\nstart_epoch = checkpoint['epoch']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:17.606849Z","iopub.execute_input":"2025-03-17T19:30:17.607225Z","iopub.status.idle":"2025-03-17T19:30:17.805984Z","shell.execute_reply.started":"2025-03-17T19:30:17.60719Z","shell.execute_reply":"2025-03-17T19:30:17.80524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Updated visualize_sample function\ndef visualize_sample(model, data_loader, device):\n    model.eval()  # Set model to evaluation mode\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Get model predictions - access the 'out' key for the main output\n            outputs = model(inputs)\n            main_output = outputs['out']  # Access the main prediction\n            predictions = main_output.argmax(dim=1)  # Get predicted class labels\n\n            # Move data back to CPU for visualization\n            inputs = inputs.cpu()\n            predictions = predictions.cpu()\n            targets = targets.cpu()\n\n            # Visualize each image in the batch\n            for i in range(len(inputs)):\n                input_img = inputs[i].permute(1, 2, 0).numpy()  # Convert to HWC format for display\n                input_img = np.clip(input_img, 0, 1)  # Clip input image to [0, 1] range\n\n                target_mask = decode_segmap(targets[i])  # Decode ground truth mask\n                predicted_mask = decode_segmap(predictions[i])  # Decode predicted mask\n\n                # Clip masks to [0, 255] and convert to uint8 if needed\n                target_mask = np.clip(target_mask * 255, 0, 255).astype(np.uint8)\n                predicted_mask = np.clip(predicted_mask * 255, 0, 255).astype(np.uint8)\n\n                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n                # Display input image\n                axs[0].imshow(input_img)\n                axs[0].set_title(\"Input Image\")\n                axs[0].axis(\"off\")\n\n                # Display ground truth mask\n                axs[1].imshow(target_mask)\n                axs[1].set_title(\"Ground Truth Mask\")\n                axs[1].axis(\"off\")\n\n                # Display predicted mask\n                axs[2].imshow(predicted_mask)\n                axs[2].set_title(\"Predicted Mask\")\n                axs[2].axis(\"off\")\n\n                plt.show()\n\n            break  # Display only the first batch for visualization\n\n# Usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvisualize_sample(model, inference_loader, device)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom torch.nn import CrossEntropyLoss\n\n# Helper function: Compute pixel accuracy\ndef calculate_accuracy(outputs, targets):\n    _, preds = torch.max(outputs, 1)\n    mask = targets != 255  # Ignore \"ignore_index\"\n    correct = (preds[mask] == targets[mask]).float().sum()\n    total = mask.float().sum()  # Count only valid pixels\n    return correct / total if total > 0 else 0  # Avoid division by zero\n\n# Helper function: Compute mean IoU\ndef calculate_mean_iou(preds, targets, n_classes):\n    preds = preds.cpu().detach().numpy().flatten()\n    targets = targets.cpu().detach().numpy().flatten()\n    mask = targets != 255  # Ignore unlabeled pixels\n\n    preds, targets = preds[mask], targets[mask]\n\n    if len(preds) == 0:\n        return 0.0  # No valid pixels\n\n    cm = confusion_matrix(targets, preds, labels=np.arange(n_classes))\n    intersection = np.diag(cm)\n    union = cm.sum(axis=1) + cm.sum(axis=0) - intersection\n    iou = intersection / (union + 1e-10)  # Avoid division by zero\n\n    return np.mean(iou[iou > 0])  # Ignore empty classes\n\n# Training function\ndef train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n    model.train()\n    running_loss = 0.0\n    running_accuracy = 0.0\n\n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device).long()\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        main_output = outputs['out']\n        aux_output = outputs.get('aux', None)  # Handle auxiliary output\n\n        # Compute losses\n        main_loss = criterion(main_output, masks)\n        aux_loss = criterion(aux_output, masks) * 0.4 if aux_output is not None else 0\n        loss = main_loss + aux_loss\n\n        # Backward pass & optimization\n        loss.backward()\n        optimizer.step()\n        # scheduler.step()\n\n        # Compute accuracy\n        accuracy = calculate_accuracy(main_output, masks)\n\n        # Logging\n        running_loss += loss.item()\n        running_accuracy += accuracy.item()\n\n        # Free memory\n        del images, masks, outputs, main_output, aux_output, loss\n        torch.cuda.empty_cache()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_accuracy = running_accuracy / len(train_loader)\n    return epoch_loss, epoch_accuracy\n\n\n# Validation loop with IoU calculation\ndef evaluate_model_with_metrics(model, val_loader, criterion, device, n_classes):\n    model.eval()\n    val_loss, val_accuracy, val_iou = 0.0, 0.0, 0.0\n\n    with torch.no_grad():  # No gradient tracking\n        for images, masks in val_loader:\n            images, masks = images.to(device), masks.to(device).long()\n\n            # Forward pass\n            outputs = model(images)\n            main_output = outputs['out'].detach()  # Detach to free memory\n            loss = criterion(main_output, masks)\n\n            # Compute metrics\n            accuracy = calculate_accuracy(main_output, masks)\n            _, preds = torch.max(main_output, 1)\n            iou = calculate_mean_iou(preds, masks, n_classes)\n\n            # Logging\n            val_loss += loss.item()\n            val_accuracy += accuracy.item()\n            val_iou += iou\n\n            # Free memory\n            del images, masks, outputs, main_output, preds\n            torch.cuda.empty_cache()\n\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_accuracy = val_accuracy / len(val_loader)\n    avg_iou = val_iou / len(val_loader)\n    \n    return avg_val_loss, avg_val_accuracy, avg_iou\n\n\n# EarlyStopping class\nclass EarlyStopping:\n    \"\"\"Stops training if validation IoU doesn't improve after a set patience.\"\"\"\n    def __init__(self, patience=5, min_delta=0.01):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_score):\n        if self.best_score is None:\n            self.best_score = val_score\n        elif val_score < self.best_score + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = val_score\n            self.counter = 0\n\n# Function to save model checkpoint\ndef save_checkpoint(state, ckpt_path):\n    torch.save(state, ckpt_path)\n    print(f\"âœ… Checkpoint saved: {ckpt_path}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:22.181763Z","iopub.execute_input":"2025-03-17T19:30:22.182133Z","iopub.status.idle":"2025-03-17T19:30:22.369394Z","shell.execute_reply.started":"2025-03-17T19:30:22.182099Z","shell.execute_reply":"2025-03-17T19:30:22.368466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nn_classes = 19\nnum_epochs = 90\npatience = 10  # Stop training if no improvement for `patience` epochs\nearly_stopping = EarlyStopping(patience=patience, min_delta=0.005)\nbest_val_iou = 0.0\nlog_path = '/kaggle/working/log_segmentation.txt'\nckpt_path = \"/kaggle/working/best_segmentation.pth\"\n\nprint(f\"ðŸ“Œ Logging to {log_path}\")\n\n# Write headers to log file\nwith open(log_path, 'w') as log_file:\n    log_file.write(\"Epoch, Train Loss, Train Acc, Val Loss, Val Acc, Val IoU\\n\")\n\n# Training loop with early stopping\nfor epoch in range(num_epochs):\n    # Train the model for one epoch\n    train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n    val_loss, val_accuracy, val_iou = evaluate_model_with_metrics(model, val_loader, criterion, device, n_classes)\n    scheduler.step(val_iou)\n\n    # Print metrics for the current epoch\n    print(f\"ðŸ”¹ Epoch {epoch + 1}/{num_epochs}\")\n    print(f\"   ðŸ“‰ Train Loss: {train_loss:.4f}, ðŸŽ¯ Train Acc: {train_accuracy:.4f}\")\n    print(f\"   ðŸ“‰ Val Loss: {val_loss:.4f}, ðŸŽ¯ Val Acc: {val_accuracy:.4f}, ðŸ”¥ Val IoU: {val_iou:.4f}\")\n\n    # Save metrics to log file\n    with open(log_path, 'a') as log_file:\n        log_file.write(f\"{epoch + 1}, {train_loss:.4f}, {train_accuracy:.4f}, {val_loss:.4f}, {val_accuracy:.4f}, {val_iou:.4f}\\n\")\n\n    # Save the best model checkpoint\n    if val_iou > best_val_iou:\n        best_val_iou = val_iou\n        save_checkpoint({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'acc': val_accuracy,\n            'val_iou': val_iou,\n            'opt_state_dict': optimizer.state_dict()\n        }, ckpt_path)\n        \n        print(f\"âœ… Best model saved at epoch {epoch + 1} (IoU: {val_iou:.4f})\")\n\n    # Early stopping check\n    early_stopping(val_iou)\n    if early_stopping.early_stop:\n        print(\"ðŸš¨ Early stopping triggered. Training stopped.\")\n        break\n\nprint(\"ðŸŽ‰ Training complete! Best IoU:\", best_val_iou)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_accuracy, val_iou = evaluate_model_with_metrics(model, val_loader, criterion, device, num_classes)\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_accuracy:.4f}, Validation IoU: {val_iou:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!/usr/src/tensorrt/bin/trtexec","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exporting to ONNX\ndummy_input = torch.randn(1, 3, 512, 1024, device='cuda')\ninput_names = [\"actual_input_1\"]\noutput_names = [\"output1\"]\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"/kaggle/working/deeplab_resnet50_base.onnx\",  # Single .onnx extension\n    verbose=False,\n    opset_version=11,\n    do_constant_folding=False,\n    input_names=input_names,\n    output_names=output_names\n)\n\n# Converting ONNX model to TRT\n! /usr/src/tensorrt/bin/trtexec --onnx=/kaggle/working/deeplab_resnet50_base.onnx --useSpinWait --avgRuns=100  --saveEngine=/kaggle/working/deeplab_resnet50_base.trt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Define dummy input with a fixed size for export, but specify dynamic dimensions\ndummy_input = torch.randn(1, 3, 1024, 2048, device='cuda')  # Example size; actual size will be dynamic\n\n# Specify input and output names\ninput_names = [\"actual_input_1\"]\noutput_names = [\"output1\"]\n\n# Define dynamic axes for batch size, height, and width\ndynamic_axes = {\n    \"actual_input_1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # Batch, height, and width are dynamic\n    \"output1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n}\n\n# Export the model with dynamic input sizes\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"/kaggle/working/deeplab_resnet50_dynamic.onnx\",  # Save as .onnx file\n    verbose=False,\n    opset_version=11,\n    do_constant_folding=False,\n    input_names=input_names,\n    output_names=output_names,\n    # dynamic_axes=dynamic_axes  # Specify dynamic axes\n)\n\n# Convert ONNX model to TensorRT with dynamic shapes\n!/usr/src/tensorrt/bin/trtexec --onnx=/kaggle/working/deeplab_resnet50_dynamic.onnx --explicitBatch  --useSpinWait --workspace=512 --avgRuns=100 --saveEngine=/kaggle/working/deeplab_resnet50_dynamic.trt\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Define dummy input with a fixed size for export, but specify dynamic dimensions\ndummy_input = torch.randn(1, 3, 512, 1024, device='cuda')  # Example size; actual size will be dynamic\n\n# Specify input and output names\ninput_names = [\"actual_input_1\"]\noutput_names = [\"output1\"]\n\n# Define dynamic axes for batch size, height, and width\ndynamic_axes = {\n    \"actual_input_1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # Batch, height, and width are dynamic\n    \"output1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n}\n\n# Export the model with dynamic input sizes\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"/kaggle/working/fcn_resnet18_fp16.onnx\",  # Save as .onnx file\n    verbose=False,\n    opset_version=11,\n    do_constant_folding=False,\n    input_names=input_names,\n    output_names=output_names,\n    # dynamic_axes=dynamic_axes  # Specify dynamic axes\n)\n\n# Convert ONNX model to TensorRT with dynamic shapes\n!/usr/src/tensorrt/bin/trtexec --onnx=/kaggle/working/fcn_resnet18_fp16.onnx --explicitBatch  --fp16 --useSpinWait --workspace=512 --avgRuns=100 --saveEngine=/kaggle/working/fcn_resnet18_fp16.trt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\n\ndef compute_class_iou(preds, targets, n_classes=19):\n    \"\"\"\n    Computes class-wise IoU for segmentation predictions.\n\n    Args:\n        preds (torch.Tensor): Predicted segmentation mask (H, W) or (batch, H, W)\n        targets (torch.Tensor): Ground truth segmentation mask (H, W) or (batch, H, W)\n        n_classes (int): Number of segmentation classes\n\n    Returns:\n        dict: IoU per class\n    \"\"\"\n    preds = preds.cpu().numpy().flatten()\n    targets = targets.cpu().numpy().flatten()\n\n    # Compute class-wise IoU\n    iou_per_class = jaccard_score(targets, preds, average=None, labels=np.arange(n_classes))\n\n    # Return a dictionary of IoU per class\n    return {f\"Class {i}\": round(iou, 4) for i, iou in enumerate(iou_per_class)}\n\n# Ensure model is in evaluation mode\nmodel.eval()\n\n# Run inference on validation set\nwith torch.no_grad():\n    for input_img, target_mask in inference_loader:  # Ensure inference_loader is defined\n        input_img, target_mask = input_img.to(device), target_mask.to(device)\n\n        # Perform inference\n        output = model(input_img)[\"out\"]\n        predicted_mask = torch.argmax(output, dim=1)  # Convert logits to class indices\n\n        # Compute class-wise IoU for this batch\n        iou_per_class = compute_class_iou(predicted_mask[0], target_mask[0], n_classes=19)\n        \n        print(\"Class-wise IoU:\", iou_per_class)\n        break  # Process only the first batch\n\ndef compute_class_weights(train_loader, n_classes=19):\n    \"\"\"\n    Computes class weights based on the frequency of each class in the dataset.\n    \n    Args:\n        train_loader (DataLoader): Training data loader\n        n_classes (int): Number of classes\n    \n    Returns:\n        torch.Tensor: Class weights for loss function\n    \"\"\"\n    class_counts = torch.zeros(n_classes)\n    \n    for _, target in train_loader:\n        target = target.view(-1)  # Flatten the masks\n        for i in range(n_classes):\n            class_counts[i] += torch.sum(target == i)\n\n    class_weights = 1.0 / (class_counts + 1e-10)  # Avoid division by zero\n    class_weights /= class_weights.sum()  # Normalize\n\n    return class_weights.to(device)\n\n# Compute weights and define weighted loss\nclass_weights = compute_class_weights(train_loader, n_classes=19)\n# criterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorrt as trt\nimport numpy as np\nimport os\n\nclass Int8Calibrator(trt.IInt8EntropyCalibrator2):\n    def __init__(self, data_loader, cache_file=\"calibration_data.cache\"):\n        super().__init__()\n        self.data_loader = data_loader\n        self.cache_file = cache_file\n        self.batch_size = 8\n        self.device_input = None\n        self.current_index = 0\n        self.data = None\n\n    def get_batch_size(self):\n        return self.batch_size\n\n    def get_batch(self, names):\n        if self.current_index >= len(self.data_loader):\n            return None\n\n        batch = self.data_loader[self.current_index]\n        batch = np.ascontiguousarray(batch.numpy().astype(np.float32))\n        self.current_index += 1\n\n        return [batch.ravel()]\n\n    def read_calibration_cache(self):\n        if os.path.exists(self.cache_file):\n            with open(self.cache_file, \"rb\") as f:\n                return f.read()\n        return None\n\n    def write_calibration_cache(self, cache):\n        with open(self.cache_file, \"wb\") as f:\n            f.write(cache)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calibrator = Int8Calibrator(\n    data_loader=calib_dataloader,  # Use full dataset for calibration\n    cache_file=\"\\kaggle\\working\\calibration_data.cache\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define dummy input with a fixed size for export, but specify dynamic dimensions\ndummy_input = torch.randn(1, 3, 512, 1024, device='cuda')  # Example size; actual size will be dynamic\n\n# Specify input and output names\ninput_names = [\"actual_input_1\"]\noutput_names = [\"output1\"]\n\n# Define dynamic axes for batch size, height, and width\ndynamic_axes = {\n    \"actual_input_1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # Batch, height, and width are dynamic\n    \"output1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n}\n\n# Export the model with dynamic input sizes\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"/kaggle/working/fcn_resnet18_int8.onnx\",  # Save as .onnx file\n    verbose=False,\n    opset_version=11,\n    do_constant_folding=False,\n    input_names=input_names,\n    output_names=output_names,\n    # dynamic_axes=dynamic_axes  # Specify dynamic axes\n)\n\n# Convert ONNX model to TensorRT with dynamic shapes\n!/usr/src/tensorrt/bin/trtexec --onnx=/kaggle/working/fcn_resnet18_int8v1.onnx --calib=/kaggle/working/calibration_data.cache --explicitBatch --int8 --useSpinWait --workspace=1024 --avgRuns=100 --saveEngine=/kaggle/working/fcn_resnet18_int8v1.trt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-----------per channel weight-----------------------------------\nquant_desc_input = QuantDescriptor(num_bits=8, calib_method='histogram')\nquant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\nquant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input)\nquant_nn.QuantAdaptiveAvgPool2d.set_default_quant_desc_input(quant_desc_input)\nquant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)\n\n# #-----------per tensor weight--------------------------------------\n# quant_desc_input = QuantDescriptor(num_bits=4, calib_method='histogram', axis=None)\n# quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n# quant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input)\n# quant_nn.QuantConvTranspose2d.set_default_quant_desc_input(quant_desc_input)\n# quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)\n\n# quant_desc_weight = QuantDescriptor(num_bits=4, calib_method='histogram', axis=None)\n# quant_nn.QuantConv2d.set_default_quant_desc_weight(quant_desc_weight)\n# quant_nn.QuantConvTranspose2d.set_default_quant_desc_weight(quant_desc_weight)\n# quant_nn.QuantLinear.set_default_quant_desc_weight(quant_desc_weight)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:28.562393Z","iopub.execute_input":"2025-03-17T19:30:28.562735Z","iopub.status.idle":"2025-03-17T19:30:28.568405Z","shell.execute_reply.started":"2025-03-17T19:30:28.562706Z","shell.execute_reply":"2025-03-17T19:30:28.567375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quant_modules.initialize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:31.493897Z","iopub.execute_input":"2025-03-17T19:30:31.494646Z","iopub.status.idle":"2025-03-17T19:30:31.498911Z","shell.execute_reply.started":"2025-03-17T19:30:31.494611Z","shell.execute_reply":"2025-03-17T19:30:31.497783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import resnet18\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom collections import OrderedDict\n\nnum_classes = 19  # Cityscapes has 19 classes\n\n# Define FCNHead manually (since torchvision does not expose it)\nclass FCNHead(nn.Sequential):\n    def __init__(self, in_channels, num_classes):\n        super().__init__(\n            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3),  # Prevents overfitting\n            nn.Conv2d(256, num_classes, kernel_size=1)\n        )\n\n# Load ResNet-18 backbone\ntry:\n    backbone = resnet18(weights=\"IMAGENET1K_V1\")  # Torchvision >= 0.13\nexcept TypeError:\n    backbone = resnet18(pretrained=True)  # Older torchvision versions\n\n# Extract only the feature layers (excluding final FC layer)\nreturn_layers = {\"layer4\": \"out\"}  # Extract feature maps from layer4\nbackbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n\n# Load pre-trained FCN-ResNet50 to initialize classifier weights\nfcn_resnet50 = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n\n# Create FCN model with ResNet-18 backbone\nq_model = torchvision.models.segmentation.fcn_resnet50(pretrained=False, num_classes=num_classes)\n\n# Replace the backbone with ResNet-18\nq_model.backbone = backbone\n\n# **Fix: Initialize a new FCNHead with correct input channels (512)**\nq_model.classifier = FCNHead(512, num_classes)  # ResNet-18 outputs 512 channels\n\n# **Fix: Do not transfer classifier weights from ResNet-50 (1024 â†’ 512 mismatch)**\n\n# **Optional:** Remove auxiliary classifier if it causes issues\nq_model.aux_classifier = None\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nq_model = q_model.to(device)\n\nprint(\"âœ… FCN-ResNet18 Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:33.504347Z","iopub.execute_input":"2025-03-17T19:30:33.505076Z","iopub.status.idle":"2025-03-17T19:30:37.192816Z","shell.execute_reply.started":"2025-03-17T19:30:33.505042Z","shell.execute_reply":"2025-03-17T19:30:37.191909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Best Model Checkpoint\ncheckpoint = torch.load(\"/kaggle/working/fcnresnet18_qat.pth\")\nq_model.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['opt_state_dict'])\nstart_epoch = checkpoint['epoch']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:39.353232Z","iopub.execute_input":"2025-03-17T19:30:39.353929Z","iopub.status.idle":"2025-03-17T19:30:39.541549Z","shell.execute_reply.started":"2025-03-17T19:30:39.353892Z","shell.execute_reply":"2025-03-17T19:30:39.540761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_amax(model, **kwargs):\n    # Load calib result\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                if isinstance(module._calibrator, calib.MaxCalibrator):\n                    module.load_calib_amax()\n                else:\n                    module.load_calib_amax(**kwargs)\n            print(F\"{name:40}: {module}\")\n    model.cuda()\n\ndef collect_stats(model, data_loader, num_batches):\n    \"\"\"Feed data to the network and collect statistics\"\"\"\n    # Enable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.disable_quant()\n                module.enable_calib()\n            else:\n                module.disable()\n\n    # Feed data to the network for collecting stats\n    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n        model(image.cuda())\n        if i >= num_batches:\n            break\n\n    # Disable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.enable_quant()\n                module.disable_calib()\n            else:\n                module.enable()\n\ndef calibrate_model(model, model_name, data_loader, num_calib_batch, calibrator, hist_percentile, out_dir):\n    \"\"\"\n        Feed data to the network and calibrate.\n        Arguments:\n            model: classification model\n            model_name: name to use when creating state files\n            data_loader: calibration data set\n            num_calib_batch: amount of calibration passes to perform\n            calibrator: type of calibration to use (max/histogram)\n            hist_percentile: percentiles to be used for historgram calibration\n            out_dir: dir to save state files in\n    \"\"\"\n\n    if num_calib_batch > 0:\n        print(\"Calibrating model\")\n        with torch.no_grad():\n            collect_stats(model, data_loader, num_calib_batch)\n\n        if not calibrator == \"histogram\":\n            compute_amax(model, method=\"max\")\n            calib_output = os.path.join(\n                out_dir,\n                F\"{model_name}-max-{num_calib_batch*data_loader.batch_size}.pth\")\n            torch.save(model.state_dict(), calib_output)\n        else:\n            for percentile in hist_percentile:\n                print(F\"{percentile} percentile calibration\")\n                compute_amax(model, method=\"percentile\")\n                calib_output = os.path.join(\n                    out_dir,\n                    F\"{model_name}-percentile-{percentile}-{num_calib_batch*data_loader.batch_size}.pth\")\n                torch.save(model.state_dict(), calib_output)\n\n            for method in [\"mse\", \"entropy\"]:\n                print(F\"{method} calibration\")\n                compute_amax(model, method=method)\n                calib_output = os.path.join(\n                    out_dir,\n                    F\"{model_name}-{method}-{num_calib_batch*data_loader.batch_size}.pth\")\n                torch.save(model.state_dict(), calib_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:30:42.85717Z","iopub.execute_input":"2025-03-17T19:30:42.857891Z","iopub.status.idle":"2025-03-17T19:30:42.869243Z","shell.execute_reply.started":"2025-03-17T19:30:42.857855Z","shell.execute_reply":"2025-03-17T19:30:42.868406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set model to evaluation mode if you're using it for inference\nq_model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir calib_cache","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Calibrate the model using max calibration technique.\nwith torch.no_grad():\n    calibrate_model(\n        model=q_model,\n        model_name=\"fcnresnet18_qat\",\n        data_loader=calib_dataloader,\n        num_calib_batch=8,\n        calibrator=\"histogram\",\n        hist_percentile=[99.9], \n        out_dir=\"/kaggle/working/calib_cache/\")\n\"\"\"[ 99.99, 99.999, 99.9999]\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/working/calib_cache/fcnresnet18_qat-percentile-99.9-64.pth\")\nq_model.load_state_dict(checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_accuracy, val_iou = evaluate_model_with_metrics(q_model, val_loader, criterion, device, num_classes)\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_accuracy:.4f}, Validation IoU: {val_iou:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nn_classes = 19\nnum_epochs = 5\npatience = 10  # Stop training if no improvement for `patience` epochs\nearly_stopping = EarlyStopping(patience=patience, min_delta=0.005)\nbest_val_iou = 0.0\nlog_path = '/kaggle/working/fcnresnet18_qat.txt'\nckpt_path = \"/kaggle/working/fcnresnet18_qat.pth\"\n\nprint(f\"ðŸ“Œ Logging to {log_path}\")\n\n# Write headers to log file\nwith open(log_path, 'w') as log_file:\n    log_file.write(\"Epoch, Train Loss, Train Acc, Val Loss, Val Acc, Val IoU\\n\")\n\n# Training loop with early stopping\nfor epoch in range(num_epochs):\n    # Train the model for one epoch\n    train_loss, train_accuracy = train_one_epoch(q_model, train_loader, criterion, optimizer, scheduler, device)\n    val_loss, val_accuracy, val_iou = evaluate_model_with_metrics(q_model, val_loader, criterion, device, n_classes)\n    scheduler.step(val_iou)\n\n    # Print metrics for the current epoch\n    print(f\"ðŸ”¹ Epoch {epoch + 1}/{num_epochs}\")\n    print(f\"   ðŸ“‰ Train Loss: {train_loss:.4f}, ðŸŽ¯ Train Acc: {train_accuracy:.4f}\")\n    print(f\"   ðŸ“‰ Val Loss: {val_loss:.4f}, ðŸŽ¯ Val Acc: {val_accuracy:.4f}, ðŸ”¥ Val IoU: {val_iou:.4f}\")\n\n    # Save metrics to log file\n    with open(log_path, 'a') as log_file:\n        log_file.write(f\"{epoch + 1}, {train_loss:.4f}, {train_accuracy:.4f}, {val_loss:.4f}, {val_accuracy:.4f}, {val_iou:.4f}\\n\")\n\n    # Save the best model checkpoint\n    if val_iou > best_val_iou:\n        best_val_iou = val_iou\n        save_checkpoint({\n            'epoch': epoch + 1,\n            'model_state_dict': q_model.state_dict(),\n            'acc': val_accuracy,\n            'val_iou': val_iou,\n            'opt_state_dict': optimizer.state_dict()\n        }, ckpt_path)\n        \n        print(f\"âœ… Best model saved at epoch {epoch + 1} (IoU: {val_iou:.4f})\")\n\n    # Early stopping check\n    early_stopping(val_iou)\n    if early_stopping.early_stop:\n        print(\"ðŸš¨ Early stopping triggered. Training stopped.\")\n        break\n\nprint(\"ðŸŽ‰ Training complete! Best IoU:\", best_val_iou)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nquant_nn.TensorQuantizer.use_fb_fake_quant = True\n# Define dummy input with a fixed size for export, but specify dynamic dimensions\ndummy_input = torch.randn(1, 3, 512, 1024, device='cuda')  # Example size; actual size will be dynamic\n\n# Specify input and output names\ninput_names = [\"actual_input_1\"]\noutput_names = [\"output1\"]\n\n# Define dynamic axes for batch size, height, and width\ndynamic_axes = {\n    \"actual_input_1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # Batch, height, and width are dynamic\n    \"output1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n}\n\n# Export the model with dynamic input sizes\ntorch.onnx.export(\n    q_model.eval(),\n    dummy_input,\n    \"/kaggle/working/fcn_resnet18_qat.onnx\",  # Save as .onnx file\n    verbose=False,\n    opset_version=13,\n    do_constant_folding=False,\n    input_names=input_names,\n    output_names=output_names,\n    # dynamic_axes=dynamic_axes  # Specify dynamic axes\n)\n\n# Convert ONNX model to TensorRT with dynamic shapes\n!/usr/src/tensorrt/bin/trtexec --onnx=/kaggle/working/fcn_resnet18_qat.onnx  --explicitBatch --int8 --useSpinWait --workspace=1024 --avgRuns=100 --saveEngine=/kaggle/working/fcn_resnet18_qat.trt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:31:15.964479Z","iopub.execute_input":"2025-03-17T19:31:15.964856Z","iopub.status.idle":"2025-03-17T19:31:34.828814Z","shell.execute_reply.started":"2025-03-17T19:31:15.964824Z","shell.execute_reply":"2025-03-17T19:31:34.827518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport tensorrt as trt\nimport torch\nimport time\nfrom sklearn.metrics import jaccard_score\n\ndef allocate_buffers(engine, context, batch_size, height, width):\n    inputs, outputs, bindings = [], [], []\n    stream = cuda.Stream()\n    for binding in range(engine.num_bindings):\n        binding_shape = (batch_size, 3, height, width) if engine.binding_is_input(binding) else (batch_size, engine.get_binding_shape(binding)[1], height, width)\n        size = trt.volume(binding_shape)\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        host_mem = cuda.pagelocked_empty(size, dtype)\n        device_mem = cuda.mem_alloc(host_mem.nbytes)\n        bindings.append(int(device_mem))\n        if engine.binding_is_input(binding):\n            inputs.append({'host': host_mem, 'device': device_mem})\n        else:\n            outputs.append({'host': host_mem, 'device': device_mem})\n    return inputs, outputs, bindings, stream\n\ndef infer_and_evaluate(engine, context, val_loader, criterion, device, n_classes):\n    total_loss, total_accuracy, total_iou, latencies = 0.0, 0.0, 0.0, []\n    total_images = 0\n    height = 1024\n    width = 2048\n    with torch.no_grad():\n        for images, masks in val_loader:\n            batch_size, channels, height, width = images.shape\n            images, masks = images.to(device), masks.to(device).long()\n            \n            # Set dynamic shape for context\n            context.set_binding_shape(0, (batch_size, channels, height, width))\n            # print(batch_size, channels, height, width)\n            # Allocate buffers based on the current shape\n            inputs, outputs, bindings, stream = allocate_buffers(engine, context, batch_size, height, width)\n            \n            # Copy images to input buffer\n            images = images.cpu().numpy().astype(np.float32)\n            np.copyto(inputs[0]['host'], images.ravel())\n            \n            # Measure latency\n            start_time = time.time()\n            cuda.memcpy_htod_async(inputs[0]['device'], inputs[0]['host'], stream)\n            context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n            cuda.memcpy_dtoh_async(outputs[0]['host'], outputs[0]['device'], stream)\n            stream.synchronize()\n            latency = time.time() - start_time\n            latencies.append(latency)\n            total_images += batch_size\n\n            # Process model output\n            main_output = torch.from_numpy(outputs[0]['host']).view(batch_size, n_classes, height, width).to(device)\n            loss = criterion(main_output, masks)\n            accuracy = calculate_accuracy(main_output, masks)\n            _, preds = torch.max(main_output, 1)\n            iou = calculate_mean_iou(preds, masks, n_classes)\n\n            total_loss += loss.item()\n            total_accuracy += accuracy.item()\n            total_iou += iou\n\n    # Calculate average metrics\n    avg_loss = total_loss / len(val_loader)\n    avg_accuracy = total_accuracy / len(val_loader)\n    avg_iou = total_iou / len(val_loader)\n    avg_latency = np.mean(latencies)\n    throughput = total_images / sum(latencies)\n\n    return avg_loss, avg_accuracy, avg_iou, avg_latency, throughput\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:31:47.459827Z","iopub.execute_input":"2025-03-17T19:31:47.460172Z","iopub.status.idle":"2025-03-17T19:31:47.594874Z","shell.execute_reply.started":"2025-03-17T19:31:47.460141Z","shell.execute_reply":"2025-03-17T19:31:47.594116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define necessary parameters\nengine_path = \"/kaggle/working/fcn_resnet18_qat.trt\"\ndevice = torch.device(\"cuda\")  # or torch.device(\"cpu\")\nn_classes = 19  # Set according to your dataset, e.g., 19 for Cityscapes\n\n# Load the TensorRT engine\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\ndef load_engine(engine_path):\n    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n        return runtime.deserialize_cuda_engine(f.read())\n\nengine = load_engine(engine_path)\n# Create an execution context from the engine\ncontext = engine.create_execution_context()\n\n# Example call to infer_and_evaluate function\navg_loss, avg_accuracy, avg_iou, avg_latency, throughput = infer_and_evaluate(\n    engine, context, inference_loader, criterion, device, n_classes\n)\n\n# Print results\nprint(\"Average Loss:\", avg_loss)\nprint(\"Average Accuracy:\", avg_accuracy)\nprint(\"Average Mean IoU:\", avg_iou)\nprint(\"Average Latency (seconds):\", avg_latency)\nprint(\"Throughput (images/second):\", throughput)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:32:03.696085Z","iopub.execute_input":"2025-03-17T19:32:03.6969Z","iopub.status.idle":"2025-03-17T19:33:00.379582Z","shell.execute_reply.started":"2025-03-17T19:32:03.696861Z","shell.execute_reply":"2025-03-17T19:33:00.378395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Decode function to convert mask to RGB (Cityscapes)\ndef decode_segmap(mask, n_classes=19, label_colours=None):\n    if label_colours is None:\n        label_colours = [\n            (128, 64, 128), (244, 35, 232), (70, 70, 70), (102, 102, 156),\n            (190, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n            (107, 142, 35), (152, 251, 152), (0, 130, 180), (220, 20, 60),\n            (255, 0, 0), (0, 0, 142), (0, 0, 70), (0, 60, 100),\n            (0, 80, 100), (0, 0, 230), (119, 11, 32)\n        ]  # 19 valid classes\n\n    if isinstance(mask, torch.Tensor):\n        mask = mask.cpu().numpy()\n\n    # Ensure ignored pixels (255) are set to zero\n    mask[mask == 255] = 0\n\n    r, g, b = np.zeros_like(mask, dtype=np.uint8), np.zeros_like(mask, dtype=np.uint8), np.zeros_like(mask, dtype=np.uint8)\n\n    for l in range(n_classes):\n        r[mask == l] = label_colours[l][0]\n        g[mask == l] = label_colours[l][1]\n        b[mask == l] = label_colours[l][2]\n\n    return np.stack([r, g, b], axis=2)  # Convert to RGB image\n\n# Function to visualize input, ground truth, and predicted masks\ndef visualize_segmentation(input_image, decoded_target=None, decoded_prediction=None):\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    # Convert input tensor to numpy for display\n    img = input_image.cpu().numpy().transpose(1, 2, 0)  # Convert [C, H, W] to [H, W, C]\n    img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n\n    axs[0].imshow(img)\n    axs[0].set_title(\"Input Image\")\n    axs[0].axis('off')\n\n    if decoded_target is not None:\n        axs[1].imshow(decoded_target)\n        axs[1].set_title(\"Ground Truth Mask\")\n        axs[1].axis('off')\n    else:\n        axs[1].imshow(np.zeros_like(img))\n        axs[1].set_title(\"Ground Truth Unavailable\")\n        axs[1].axis('off')\n\n    if decoded_prediction is not None:\n        axs[2].imshow(decoded_prediction)\n        axs[2].set_title(\"Predicted Mask\")\n        axs[2].axis('off')\n    else:\n        axs[2].imshow(np.zeros_like(img))\n        axs[2].set_title(\"Prediction Unavailable\")\n        axs[2].axis('off')\n\n    plt.show()\n\n# Function to perform inference using TensorRT and visualize predictions\ndef visualize_trt_predictions(engine, context, inference_loader, device, n_classes=19):\n    height, width = 512, 1024\n\n    with torch.no_grad():\n        for input_img, target_mask in inference_loader:\n            input_img, target_mask = input_img.to(device), target_mask.to(device)\n            batch_size, channels, h, w = input_img.shape\n\n            # Set TensorRT input shape dynamically\n            context.set_binding_shape(0, (batch_size, channels, height, width))\n\n            # Allocate buffers\n            inputs, outputs, bindings, stream = allocate_buffers(engine, context, batch_size, height, width)\n\n            # Copy image to input buffer\n            images_np = input_img.cpu().numpy().astype(np.float32)\n            np.copyto(inputs[0]['host'], images_np.ravel())\n\n            # Perform inference\n            cuda.memcpy_htod_async(inputs[0]['device'], inputs[0]['host'], stream)\n            context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n            cuda.memcpy_dtoh_async(outputs[0]['host'], outputs[0]['device'], stream)\n            stream.synchronize()\n\n            # Process model output\n            main_output = torch.from_numpy(outputs[0]['host']).view(batch_size, n_classes, height, width).to(device)\n            _, predicted_mask = torch.max(main_output, 1)  # Get predicted class indices\n\n            # Select single image from batch\n            input_image = input_img[0]\n            target_mask = target_mask[0]\n            predicted_mask = predicted_mask[0]\n\n            # Decode segmentation masks\n            decoded_target = decode_segmap(target_mask, n_classes=n_classes) if target_mask is not None else None\n            decoded_prediction = decode_segmap(predicted_mask, n_classes=n_classes) if predicted_mask is not None else None\n\n            # Visualize the segmentation results\n            visualize_segmentation(input_image, decoded_target, decoded_prediction)\n            break  # Show only one batch\n\n# Call the visualization function with the TensorRT engine\nvisualize_trt_predictions(engine, context, inference_loader, device, n_classes=19)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:35:07.91028Z","iopub.execute_input":"2025-03-17T19:35:07.910665Z","iopub.status.idle":"2025-03-17T19:35:09.262459Z","shell.execute_reply.started":"2025-03-17T19:35:07.910634Z","shell.execute_reply":"2025-03-17T19:35:09.261544Z"}},"outputs":[],"execution_count":null}]}